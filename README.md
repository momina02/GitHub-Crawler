# ğŸ•¸ï¸ GitHub Repositories Crawler 

A fully automated system that crawls **popular GitHub repositories**, stores them in **MySQL**, refreshes them daily, and exports a clean **CSV snapshot** â€” all powered by **GitHub Actions + GraphQL API**.

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![GitHub API](https://img.shields.io/badge/GitHub%20API-181717?style=for-the-badge&logo=github&logoColor=white)
![Requests](https://img.shields.io/badge/Requests-000000?style=for-the-badge&logo=python&logoColor=white)
![JSON](https://img.shields.io/badge/JSON-333333?style=for-the-badge&logo=json&logoColor=white)
![Automation](https://img.shields.io/badge/Automation-FF6F00?style=for-the-badge)
![Data%20Engineering](https://img.shields.io/badge/Data%20Engineering-4CAF50?style=for-the-badge)

## ğŸ”¥ Tech Stack

**Languages & Tools:**
`Python` â€¢ `GitHub GraphQL API` â€¢ `MySQL` â€¢ `GitHub Actions` â€¢ `Automation` â€¢ `Data Engineering` â€¢ `Docker` â€¢ `CSV Export`

## ğŸ¯ What This Project Does

This crawler automatically:

* Fetches **popular repositories** from GitHub using the **GraphQL API**
* Stores & updates repo metadata in **MySQL**
* Handles **pagination**, **batch inserts**, and **refresh cycles**
* Exports the final dataset as **repos.csv**
* Runs **daily** with full automation via **GitHub Actions**

# âš™ï¸ Overall Workflow

1. **ğŸ§© Setup `.env`**

   * Contains MySQL credentials + crawler configs
   * GitHub Actions runner initializes MySQL Docker container

2. **ğŸ›¢ï¸ Database Setup**

   * `db.py` ensures `github_data` DB + `repos` table exists
   * Auto-creates DB & schema if missing

3. **ğŸ•·ï¸ Fetching Repos**

   * `main.py` / `init_crawler.py` triggers `GitHubGraphQLCrawler`
   * Fetches repos in **6-month intervals** (star-based search)
   * Uses **GraphQL cursors** for pagination
   * Inserts/updates rows in **batch mode**

4. **ğŸ”„ Refreshing Existing Data**

   * Updates stars, forks, timestamps
   * Keeps `crawled_at` always fresh

5. **ğŸ“¤ Exporting**

   * Full `repos` table exported to `repos.csv`
   * CSV uploaded as Actions **artifact**

6. **ğŸ¤– Automation**

   * Daily GitHub Actions run
   * Ensures fresh DB + updated CSV snapshot

# ğŸ§­ Visual Workflow (Simplified)

```
GitHub Actions Trigger
        |
        v
   MySQL Container
        |
        v
Load .env -> initialize DB (db.py)
        |
        v
Run main.py -> GitHub GraphQL Crawler
        |       -> Fetch popular repos
        |       -> Batch insert/update MySQL
        |
        v
Refresh existing repos
        |
        v
Export MySQL table -> repos.csv
        |
        v
Upload CSV Artifact
```

---

# ğŸ“ File-Level Workflow

### **ğŸ“Œ db.py**

* Loads env variables (host, port, user, password, DB)
* Creates the MySQL DB + `repos` table
* Provides `get_conn()` for other modules
* Ensures clean schema creation

### **ğŸ“Œ github_api.py**

* Contains `GitHubGraphQLCrawler`
* Handles **authentication**, **GraphQL queries**, **pagination**
* `fetch_popular_repos()` retrieves repos in 6-month intervals
* `fetch_repo_details()` updates a single repo
* Yields structured repository metadata

### **ğŸ“Œ init_crawler.py**

* Bulk fetch + batch insert logic
* Builds formatted MySQL rows
* Handles API rate limiting
* Ideal for initial large-scale data ingestion

### **ğŸ“Œ main.py**

* Main automation runner
* Detects if DB is empty â†’ performs initial crawl
* Refreshes existing repos (stars, forks, updated_at)
* Cleans & exports `repos.csv`

### **ğŸ“Œ crawler.yaml (GitHub Actions)**

* Runs daily or manually
* Starts MySQL container
* Creates `.env` automatically
* Initializes DB + runs crawler
* Exports & uploads CSV artifact

# ğŸ“Š Summary

This system:

* ğŸ›¢ï¸ Creates & manages a MySQL repo database
* ğŸ”— Fetches GitHub repos via **GraphQL**
* ğŸ§± Stores + updates records in batches
* ğŸ•’ Refreshes daily using **GitHub Actions**
* ğŸ“¤ Outputs a clean CSV snapshot of  popular repos

Fully automated. Zero manual effort.

# ğŸš€ Future Enhancements

### **ğŸ“Œ Scaling to 500,000+ Repositories**

* Parallel scripts for different:

  * date ranges
  * languages
  * star ranges
* Split workloads to accelerate crawling
* Add async tasks + queue-based operations

### **ğŸ“Œ Future Database Schema**

* Break into multiple tables:

  * `repos`
  * `issues`
  * `comments`
  * `stars`
  * `contributors`
* Cleaner, normalized, analytics-friendly schema
* All linked via `repo_id`
